{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1436b-26fe-4f12-bb56-0090ead7a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from agq.metrics import get_rank_error, get_hit_ratio, get_ndcg, get_spearman\n",
    "from constants import DATA_STREAM_FOLDER, RANKING_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d4988-3073-4567-8615-eb1b72c2d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**{\n",
    "    'path_to_scenario': 'XX',  #change me\n",
    "    'streams_root': 'card_10000_202303010000YY',\n",
    "    'aggregation_level': 'file',\n",
    "    'k': -1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cc4e6-1350-4138-b1b3-76354abf8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params\n",
    "with open(f'{args.path_to_scenario}/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "    params.update(vars(args))\n",
    "    args = argparse.Namespace(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de56e1-2f5e-4415-9e3e-9c6493dcb812",
   "metadata": {},
   "source": [
    "#### Specify the scenarios you want to compare, and the datasets. Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2759728-c101-428d-abf4-07e05265fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_roots = ['card_1000_20230302000000'] # from your local filesystem; name depends on when run\n",
    "scenario_paths = ['../synthetics/scenarios/scenario_{}'.format(i) for i in range(6)]  # indices depend on order in which they are run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5780c5-499a-4466-9c00-592bbab7d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['rank_error', 'spearman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4ceb2-4619-4360-bbbe-d7514d9b161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {} \n",
    "for root in stream_roots:\n",
    "    args.streams_root = root\n",
    "    for met in metrics:\n",
    "        dataframe = defaultdict(list)\n",
    "        for scenario in scenario_paths:\n",
    "            args.path_to_scenario = scenario\n",
    "            print(args)\n",
    "            for topK in [50]:\n",
    "                streams_filenames = os.listdir(f'{args.data_folder}{DATA_STREAM_FOLDER}/')\n",
    "                if args.streams_root != '':\n",
    "                    streams_filenames = [e for e in streams_filenames if args.streams_root in e]\n",
    "\n",
    "                streams_filenames = sorted(streams_filenames)\n",
    "\n",
    "                scores = defaultdict(list)\n",
    "                for filename in tqdm(streams_filenames):\n",
    "                    stream_name = filename.split('.csv')[0]\n",
    "                    truth_ranking = pd.read_csv(f'{args.data_folder}{RANKING_FOLDER}/{filename}')\n",
    "\n",
    "                    filename_scores, counter = defaultdict(list), 0\n",
    "                    while os.path.isfile(f'{args.path_to_scenario}/{stream_name}_ranking_{counter}.csv'):\n",
    "                        estimated_ranking = pd.read_csv(\n",
    "                            f'{args.path_to_scenario}/{stream_name}_ranking_{counter}.csv')\n",
    "                        \n",
    "                        if topK != -1:\n",
    "                            estimated_ranking = estimated_ranking.iloc[:topK - 1]\n",
    "\n",
    "                        filename_scores['rank_error'].append(\n",
    "                            get_rank_error(truth_ranking=truth_ranking, estimated_ranking=estimated_ranking, weighted=False)\n",
    "                        )\n",
    "                        filename_scores['spearman'].append(\n",
    "                            get_spearman(truth_ranking=truth_ranking, estimated_ranking=estimated_ranking, weighted=False)\n",
    "                        )\n",
    "                        counter += 1\n",
    "\n",
    "                    for k, v in filename_scores.items():\n",
    "                        v_array = np.array(v)\n",
    "                        scores[k].append((v_array.mean(), v_array.std()))\n",
    "\n",
    "                dataframe[met].append([\"{:.2f} ({:.2f})\".format(mean, std) for mean, std in scores[met]])\n",
    "        res[(root, met)] = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34ddec-2403-4a0c-a05a-4a3f435b0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca671f92-b18b-4149-9057-3a2ce3a6fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stream_roots:\n",
    "    for m in metrics:\n",
    "        d = pd.DataFrame(res[(s, m)][m], index  =['UP-F', 'UP-T', 'UN-F','UN-T', 'PURE', 'CDF'] ).transpose()\n",
    "        print(d[['PURE', 'CDF', 'UN-F','UP-F',   'UN-T',  'UP-T']].to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn",
   "language": "python",
   "name": "syn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
